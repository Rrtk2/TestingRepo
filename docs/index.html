<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Prediction of boiling points using WikiData and Machine learning techniques</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">MSB1015 - Scientific Programming - Assignment 2</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="version.html">Used packages</a>
</li>
<li>
  <a href="raw.html">Code</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Prediction of boiling points using WikiData and Machine learning techniques</h1>

</div>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>This GitHub Page is a final product of assignment 2, requested by the course MSB1015 (Scientific Programming). The goal is to create a GitHub Page (notebook) for the following analysis. The boiling points of alkenes need to be predicted based on data from WikiData; Smiles are used to enrich the data using the rcdk package in R, afterward, machine learning is applied. See the <a href="https://github.com/Rrtk2/MSB1015-Assignment-2">GitHub repository</a> for more details.</p>
</div>
<div id="programming-language" class="section level2">
<h2>Programming language</h2>
<ul>
<li>R</li>
</ul>
</div>
<div id="programming-apporaches" class="section level2">
<h2>Programming apporaches</h2>
<ul>
<li>WikiData query</li>
<li>rcdk (r package)</li>
<li>machine learning using caret (r package)</li>
</ul>
</div>
<div id="workflow" class="section level2">
<h2>Workflow</h2>
<p>The following workflow is applied, coherent to the script blocks:</p>
<ol style="list-style-type: decimal">
<li><p>Install packages</p></li>
<li><p>Define user settings</p></li>
<li><p>Set user functions</p></li>
<li><p>Define WikiData query call &amp; extract data</p></li>
<li><p>Convert units to Kelvin</p></li>
<li><p>(Optional) filtering and data cleanup</p></li>
<li><p>Visualize raw data</p></li>
<li><p>Data enrichment (using rcdk)</p></li>
<li><p>Latent variable filtering</p></li>
<li><p>Data subsetting for machine learning</p></li>
<li><p>Machine learning (PLS model)</p></li>
<li><p>Machine learning (RandomForest model)</p></li>
</ol>
<div id="script-block-1-install-packages" class="section level4">
<h4>Script block 1: INSTALL PACKAGES</h4>
<p>This block installs all packages which are required during this algorithm. It iteratively checks if they are installed, installs if needed, and load them afterward.</p>
<pre class="r"><code>    # Install packages if needed and load, or just load packages
    if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE))
      install.packages(&quot;BiocManager&quot;, ask = F)

    # Insert all packages in requiredpackages
    requiredpackages &lt;-
      c(&quot;WikidataQueryServiceR&quot;,&quot;ggplot2&quot;,&quot;backports&quot;,&quot;rJava&quot;,&quot;rcdk&quot;,&quot;pls&quot;,&quot;randomForest&quot;,
      &quot;gplots&quot;,&quot;curl&quot;,&quot;data.table&quot;,&quot;caret&quot;,&quot;ggfortify&quot;,&quot;tidyverse&quot;)
        
    for (i in requiredpackages) {
        if (!requireNamespace(i, quietly = TRUE))
            BiocManager::install(i, ask = F, dependencies = c(&quot;Depends&quot;, &quot;Imports&quot;))
        require(as.character(i), character.only = TRUE)
        print(i)
    }</code></pre>
<pre><code>## [1] &quot;WikidataQueryServiceR&quot;
## [1] &quot;ggplot2&quot;
## [1] &quot;backports&quot;
## [1] &quot;rJava&quot;
## [1] &quot;rcdk&quot;
## [1] &quot;pls&quot;
## [1] &quot;randomForest&quot;
## [1] &quot;gplots&quot;
## [1] &quot;curl&quot;
## [1] &quot;data.table&quot;
## [1] &quot;caret&quot;
## [1] &quot;ggfortify&quot;
## [1] &quot;tidyverse&quot;</code></pre>
</div>
<div id="script-block-2-settings" class="section level4">
<h4>Script block 2: SETTINGS</h4>
<p>In this script some parameters can be changed, these are located in this block. This way changing parameters is easy and robust.</p>
<pre class="r"><code>    # General settings
    options(stringsAsFactors    = F)

    # Select 80% for train, 20% for test
    trainingTestRatio = 0.8
    linearAlkanesOnly = 0  # 1 = yes; 0 = no.

    # Setting seed to keep consistent results 
    set.seed(1)

    # The variable &#39;query&#39; contains the query call; make sure the query text is within the &quot;&#39;&quot; and &quot;&#39;&quot;.
    query = &#39;

    SELECT DISTINCT ?comp ?compLabel ?bp ?bpUnitLabel ?CC WHERE {
      ?comp wdt:P31/wdt:P279* wd:Q41581 ;
            p:P2102 [
              ps:P2102 ?bp ;
              psv:P2102/wikibase:quantityUnit  ?bpUnit
            ] .
            ?comp wdt:P233 ?CC .
      SERVICE wikibase:label { bd:serviceParam wikibase:language &quot;[AUTO_LANGUAGE],en&quot;. }
    }

    &#39;</code></pre>
</div>
<div id="script-block-3-functions" class="section level4">
<h4>Script block 3: FUNCTIONS</h4>
<p>To assess the performance of the models at later hand, these functions have been made. The Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) are often used to compare models of similar function.</p>
<pre class="r"><code>    # Root Mean Squared Error
    RMSE = function(yact, ypred){
      sqrt(mean((yact - ypred)^2))
    }

    # Mean Absolute Error 
    MAE = function(yact, ypred){
      mean(abs(yact - ypred))
    }</code></pre>
</div>
<div id="script-block-4-query-call" class="section level4">
<h4>Script block 4: QUERY CALL</h4>
<p>This block gets the data from WikiData. As stated before, the goal is to extract boiling points from alkenes. This query call extracts the following pieces of information: unique alkene name, boiling point, boiling point unit and concatenated smiles. It is essential the concatenated smiles are extracted, as these will be used later to enrich the data. All this information is stored in a dataframe called ‘dataObj’.</p>
<pre class="r"><code>    # Set endpoint for wikidata
    endpoint = &quot;https://query.wikidata.org/bigdata/namespace/wdq/sparql&quot;

    # Get the results
    queryResults = query_wikidata(query)</code></pre>
<pre><code>## 134 rows were returned by WDQS</code></pre>
</div>
<div id="script-block-5-boilingpoint-unit-conversion" class="section level4">
<h4>Script block 5: BOILINGPOINT UNIT CONVERSION</h4>
<p>As the query extracts <em>all</em> boiling points, several different units might be used for a single alkene. Thus, these need to be checked and converted to Kelvin, as this is a international unit. However, during the development of this script only two other units were found, Celsius and Fahrenheit. This means when another unit is introduced, the algorithm cannot detect it and will use this unit as outlier. To overcome this, after conversion all the samples containing a ‘Kelvin’ unit will be used, while others are removed from the dataset. Finally, ‘dataObj’ is subsetted to contain the alkene name, boiling point and concatenated smiles.</p>
<pre class="r"><code>    #C to Kelvin:
    # 0°C + 273.15 = 273,15K
    indexCelciusResults = which(queryResults$bpUnitLabel==&quot;degree Celsius&quot;)
    for( i in 1:length(indexCelciusResults)){
        queryResults[indexCelciusResults[i],]$bp = (queryResults[indexCelciusResults[i],]$bp + 273.15)
        queryResults[indexCelciusResults[i],]$bpUnitLabel = &quot;kelvin&quot;
    }

    #F to Kelvin:
    # (0°F − 32) × 5/9 + 273.15 = 255,372K
    indexFahrenheitResults = which(queryResults$bpUnitLabel==&quot;degree Fahrenheit&quot;)
    for( i in 1:length(indexFahrenheitResults)){
        queryResults[indexFahrenheitResults[i],]$bp= (queryResults[indexFahrenheitResults[i],]$bp - 32) * (5/9) + 273.15
        queryResults[indexFahrenheitResults[i],]$bpUnitLabel = &quot;kelvin&quot;
    }

    #additional failsafe if other metrics than Celsius and Fahrenheit are added (other metrics are removed)
    queryResults = queryResults[which(queryResults$bpUnitLabel==&quot;kelvin&quot;),]

    # Make backup
    queryResultsBACKUP = queryResults

    # Finalize data structure
    queryResults = data.frame(queryResultsBACKUP$compLabel, queryResultsBACKUP$bp, queryResultsBACKUP$CC)
    names(queryResults) = c(&quot;Comp&quot;,&quot;bp&quot;,&quot;CC&quot;)</code></pre>
</div>
<div id="script-block-6-filters-and-outlier-handling" class="section level4">
<h4>Script block 6: FILTERS AND OUTLIER HANDLING</h4>
<p>The extracted data can be filtered on linear alkenes only (no branches), this can be changed in settings. It simplifies the model as it does not need to take the branching in account. After evaluation of the data (which happens in block 7) it was observed two alkenes deviated from the observable trend line. This was because one alkene (hexatriacontane) contained boiling point data under pressurized condition, altering the boiling point compared to the rest. The second alkene (Dooctacontane) has a lower boiling point temperature for no clear reason; when looking this up in <a href="http://www.chemspider.com/Chemical-Structure.455910.html">ChemSpider</a>, it is observed the boiling point is 684.9±18.0 °C while WikiData states 608.7 °C. The two alkanes are locally corrected as these need looking in to.</p>
<pre class="r"><code>    if(linearAlkanesOnly == 1){
        queryResults = queryResults[-grep(pattern = &quot;\\(&quot;,x = queryResults$CC),]
    }

    # hexatriacontane 770.15 K (497 C); in wikidata under pressurised condition!
    if(!length(queryResults$Comp==&quot;hexatriacontane&quot;)==0){
        queryResults$bp[queryResults$Comp==&quot;hexatriacontane&quot;] = 770.15
    }
    # Dooctacontane 958.05 K (684.9 C); IN WIKIDATA AS 881.85 K
    if(!length(queryResults$Comp==&quot;Dooctacontane&quot;)==0){
        queryResults$bp[queryResults$Comp==&quot;Dooctacontane&quot;] = 958.05
    }

    # phytane 612.32 K (322.4 C); IN WIKIDATA AS 442.65 K at 760 torr which should be (322.4 C) 612.32 K.
    if(!length(queryResults$Comp==&quot;phytane&quot;)==0){
        queryResults$bp[queryResults$Comp==&quot;phytane&quot;] = 612.32
    }</code></pre>
</div>
<div id="script-block-7-data-overview" class="section level4">
<h4>Script block 7: DATA OVERVIEW</h4>
<p>After correction / filtering the data, it is visualized in this block. It indicates an exponential relationship between boiling point and length of the alkene (based on the carbon atom backbone). This is a indication that a ‘simple’ model should be able to pick up the variables involved to determine the boiling point. The histogram indicates the boiling point frequency distribution, ideally, this would be normally distributed. It can be observed that it is <em>not</em> normally distributed; and should be taken in account.</p>
<pre class="r"><code>    # Get a general idea of how the data looks; disregarding branch effects; amount of C in compound linked to BP
    CClength_crude = nchar(gsub(pattern = &quot;\\)&quot;,replacement = &quot;&quot;,x = gsub(pattern = &quot;\\(&quot;,replacement = &quot;&quot;,x = queryResults$CC)))
    plotCClength = CClength_crude[order(CClength_crude)]
    plotBP = queryResults$bp[order(CClength_crude)]

    # Should result in a exponential function-like graph
    plot(plotCClength,plotBP,main = &quot;Carbon - boilingpoint relation&quot;,xlab = &quot;Amount of carbon atoms in alkene&quot;,ylab = &quot;Boiling point (Kelvin)&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>    # Show how the data is distributed (focussing on bp)
    hist(queryResults$bp,breaks=20,main = &quot;Boiling point frequency distribution&quot;,xlab = &quot;Boiling point (Kelvin)&quot;,ylab = &quot;Frequency&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
</div>
<div id="script-block-8-data-enrichment" class="section level4">
<h4>Script block 8: DATA ENRICHMENT</h4>
<p>To build a model, variables are required to reflect the selected molecule. Several aspects can be derived from the concatenated smiles directly, such as the amount of (C)arbon and (H)ydrogen atoms. However, when looking into molecules many more things can be stated, such as electronegativity or 3d-structure. The <a href="https://cran.r-project.org/web/packages/rcdk/index.html">RCDK package</a> in R uses concatenated smiles to determine several molecular descriptors. This information can be stored in a data frame per alkene molecule, which becomes the input for machine learning. This is possible as the output <em>y</em> (boiling point) is extracted from WikiData, and variables <em>v(1)..v(n)</em> are the obtained molecular descriptors. This dataframe is called ‘descs’, containing many variables.</p>
<pre class="r"><code>    smilesParser &lt;- get.smiles.parser()
    descCategories &lt;- get.desc.categories()

    for( i in 1:length(queryResults$CC)){

        # Get smiles from result query and add information
        selectedSmilesData &lt;- parse.smiles(queryResults$CC[i])[[1]]
        convert.implicit.to.explicit(selectedSmilesData)
        #formula &lt;- get.mol2formula(selectedSmilesData,charge=0)


        # Store the found information
        #queryResults$formula[i] = {formula} # S4 object cannot be transferred nicely
        #queryResults$mass[i] = formula@mass
        #queryResults$string[i] = formula@string
        #queryResults$charge[i] = formula@charge

        # M/z values 
        #queryResults$isotopes[i] = {get.isotopes.pattern(formula,minAbund=0.1)}

        # Fingerprint? values
        #queryResults$fingerprint[i] = {get.fingerprint(selectedSmilesData = selectedSmilesData)}

        # Create a dataframe which contains all info possible to extract using the descriptors
        rowSmileDescData = queryResults$Comp[i]
        for(o in 1:5){
            dn &lt;- get.desc.names(descCategories[o])
            rowSmileDescData = cbind(rowSmileDescData, eval.desc(selectedSmilesData, dn))
        }
        
        # This is done now as it will break if descriptors change (amount)
        if(exists(&quot;dfSmilesDescData&quot;)){
            dfSmilesDescData[i,] = rowSmileDescData
            }else{
            dfSmilesDescData = rowSmileDescData
        }

    }

    # Make backup from data, easy for testing (resetting)
    mydataBACKUP = dfSmilesDescData
    dfSmilesDescData = mydataBACKUP

    # Remove component names (as it will mess up futher processing)
    dfSmilesDescData = dfSmilesDescData[,-1]</code></pre>
</div>
<div id="script-block-9-latent-variable-filter" class="section level4">
<h4>Script block 9: LATENT VARIABLE FILTER</h4>
<p>Because ‘descs’ contains many variables, the <em>curse of dimensionality</em> is a phenomena stating the amount of variables is bigger than the amount of samples, leading to infinite results (cannot predict). To account for this several (crude) steps are taken, first all columns containing NAs are removed. Second, the descriptors are correlated with the boiling point to indicate correlated variables. At last, the top 15 (abs) correlating variables are selected and kept while all other variables are removed. As there are around 130 samples and 15 variables, a prediction is possible. A dataframe called ‘my_data’ is created containing: the amount of carbons in the backbone <em>n</em>, the 15 top correlated variables <em>v(1..15)</em> and the expected output <em>y</em>.</p>
<pre class="r"><code>    # Remove NAs n stuff
    dfSmilesDescData &lt;- dfSmilesDescData[, !apply(dfSmilesDescData, 2, function(x) any(is.na(x)) )]
    dfSmilesDescData &lt;- dfSmilesDescData[, !apply( dfSmilesDescData, 2, function(x) length(unique(x)) == 1 )]

    if(T){
        # Correlate the descriptors with the boiling point; if these are linked, they should add some info
        corMatrix = cor(dfSmilesDescData , queryResults$bp)
        corMatrix = as.data.frame(corMatrix[order(abs(corMatrix),decreasing = T),])

        # select first 15 components:
        componentNames = rownames(corMatrix)[1:15]
        dfSmilesDescData = dfSmilesDescData[,match(colnames(dfSmilesDescData), x = componentNames)]
    }

    # - BLOCKED - Old method, keepin this inside for further reference 
    if(F){
        dfSmilesDescData = mydataBACKUP
        dfSmilesDescData = dfSmilesDescData[,-1]

        # crude way to extract &#39;important&#39; features based on correlation
        dfSmilesDescData &lt;- dfSmilesDescData[, !apply(dfSmilesDescData, 2, function(x) any(is.na(x)) )]
        dfSmilesDescData &lt;- dfSmilesDescData[, !apply( dfSmilesDescData, 2, function(x) length(unique(x)) == 1 )]
        r2 &lt;- which(cor(dfSmilesDescData)^2 &gt; .9, arr.ind=TRUE) # when keeping this high, the prediction improves
        r2 &lt;- r2[ r2[,1] &gt; r2[,2] , ]
        dfSmilesDescData &lt;- dfSmilesDescData[, -unique(r2[,2])]
    }
    # - BLOCKED -

    # should contain nAtomLAC; the amount of c atoms
    if(!exists(&quot;dfSmilesDescData$nAtomLAC&quot;)){

        dfSmilesDescData = cbind(dfSmilesDescData, dfSmilesDescData$nAtomLAC)
        names(dfSmilesDescData)[dim(dfSmilesDescData)[2]] = &quot;n&quot;
        }else{
        names(dfSmilesDescData)[names(dfSmilesDescData)==&quot;dfSmilesDescData$nAtomLAC&quot;] = &quot;n&quot;
    }

    # Store resulting input file in dfInputML; as it is the input for ML
    dfInputML = dfSmilesDescData

    # Finish prepare data
    # Add and define the &#39;to be predicted&#39; column
    indexBoilPoint = ncol(dfInputML)+1
    dfInputML[,indexBoilPoint] = queryResults$bp
    names(dfInputML)[indexBoilPoint] = &#39;BoilPoint&#39;

    # Ordered data is required later
    dfInputML = dfInputML[order(dfInputML$BoilPoint),]</code></pre>
</div>
<div id="script-block-10-data-subsetting-for-machine-learning" class="section level4">
<h4>Script block 10: DATA SUBSETTING FOR MACHINE LEARNING</h4>
<p>To create robust (machine learning) models several aspects need to be accounted for. Overfitting is the process of fitting the model perfectly to <em>this</em> dataset, while another dataset might not fit that well. A balance must be struck between robustness and minimal error. This is why the dataset is split into a training and test set, by evaluating the model performance on both sets, the overfitting and robustness van be evaluated. This block of code splits the data in a test and training set, using a density-biased random sampling approach. As stated before, the samples are not uniformly distributed, thus, a random sampling approach <em>will</em> select more samples with higher boiling points, shifting the model focus to higher boiling points rather than uniform.</p>
<pre class="r"><code>    # Define variables 
    sample.length = length(dfInputML[,1])

    # Higher probability to select lower (underrepresented) samples; 
    # improves range of model, increases fit and prediction power.
    # Ranges from 1 (select) to 0.5 (chance)
    sample.biasprob = 1 - 1:sample.length /max(sample.length )/2 

    # Make data objects
    samples.upper = sample(sample.length , floor(length(dfInputML[,1])*trainingTestRatio),prob = sample.biasprob ) #get all unique samples (not frequecies) and sample 80%
    #plot(samples.upper[order(samples.upper)])
    samples.total = (1:length(dfInputML[,1])) # all unique samples (not frequecies)
    samples.lowerl = samples.total[!samples.total %in% samples.upper]  #which samples are sampled

    # Subset data
    data.train = dfInputML[samples.upper,]#contains all upper 
    data.train = as.data.frame(data.train)
    data.test = dfInputML[samples.lowerl,]#contains all lower 
    data.test = as.data.frame(data.test)

    # Remove nas
    data.train=data.train[!is.na(data.train[,1]),]
    data.test=data.test[!is.na(data.test[,1]),]

    # Create standardized object
    xNN = data.train[,-(indexBoilPoint)]
    yNN = data.train[,indexBoilPoint]
    dat = data.frame(xNN, y = yNN)

    # Remove samples with in datasets n &lt; 1
    data.train = dat[dat$n&gt;0,]
    yactual.train = data.train$y
    data.test = data.test[data.test$n&gt;0,]
    yactual.test = data.test$BoilPoint</code></pre>
</div>
<div id="script-block-11-pls-model" class="section level4">
<h4>Script block 11: PLS MODEL</h4>
<p>A PLS model was created using the <a href="http://topepo.github.io/caret/index.html">Caret</a> package in R, which is a framework for a variety of machine learning approaches. (10) k-fold cross validation was applied to increase the model performance as it estimates the best latent variables which should be used. When evaluating the model’s performance, RMSE and MAE is used in addition to a predict vs actual plot. These are calculated for training and test set, to indicate robustness and overfitting.</p>
<pre class="r"><code>    # Define training control method; 10 - k - cross validation
    train_control &lt;- trainControl(method=&quot;cv&quot;, number=10)

    # Train the model
    model &lt;- train(y~., data=data.train, trControl=train_control, method=&quot;pls&quot;)

    # Find out what model is best
    print(model)</code></pre>
<pre><code>## Partial Least Squares 
## 
## 106 samples
##  16 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 96, 97, 96, 96, 94, 95, ... 
## Resampling results across tuning parameters:
## 
##   ncomp  RMSE      Rsquared   MAE     
##   1      79.17496  0.9106916  68.99771
##   2      55.89763  0.9587178  43.19882
##   3      16.73711  0.9964133  13.26975
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was ncomp = 3.</code></pre>
<pre class="r"><code>    # Find out most important variables
    Varimportance = varImp(model)
    cat(paste(&quot;Best model fit with&quot;, model$bestTune, &quot;latent components \n&quot;))</code></pre>
<pre><code>## Best model fit with 3 latent components</code></pre>
<pre class="r"><code>    cat(paste(&quot;Latent components:&quot;,paste(rownames(Varimportance$importance)[order(decreasing = T,Varimportance$importance$Overall)][1:model$bestTune[1,]],collapse = &quot;, &quot;),&quot;\n&quot;))</code></pre>
<pre><code>## Latent components: VAdjMat, Kier1, khs.ssCH2</code></pre>
<pre class="r"><code>    plot(Varimportance, main=&quot;Varible importance in PLS model \n&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>    # Predict test set
    ypredCARET.pls.test &lt;- model %&gt;% predict(data.test)

    # Root mean squared error
    RMSE.pls.test = RMSE(yactual.test, ypredCARET.pls.test)
    RMSE.pls.test</code></pre>
<pre><code>## [1] 16.55832</code></pre>
<pre class="r"><code>    # Results in: 16.55

    # Mean absolute error
    MAE.pls.test = MAE(yactual.test, ypredCARET.pls.test)
    MAE.pls.test</code></pre>
<pre><code>## [1] 14.99758</code></pre>
<pre class="r"><code>    # Results in: 14.99

    # Plot ypred vs yactual of test data
    plot(yactual.test, ypredCARET.pls.test,
        xlab=&quot;Observed BP test set&quot;, ylab=&quot;Predicted BP test set&quot;,
        pch=19, xlim=c(0, ceiling(max(yNN)*1.1)), ylim=c(0, ceiling(max(yNN)*1.1)),main=&quot;Prediction error test set (PLS model)&quot;)
        abline(0,1, col=&#39;red&#39;)
        text(200,ceiling(max(yNN)*1.1),paste(&quot;RMSE: &quot;,RMSE.pls.test))
        text(200,ceiling(max(yNN)*1.1)-50,paste(&quot;MAE: &quot;,MAE.pls.test))</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
<pre class="r"><code>    # Predict training data; check overfitting
    ypredCARET.pls.train &lt;- model %&gt;% predict(data.train)

    # Root mean squared error
    RMSE.pls.train = RMSE(yactual.train, ypredCARET.pls.train)
    RMSE.pls.train</code></pre>
<pre><code>## [1] 16.0855</code></pre>
<pre class="r"><code>    # Results in: 16.08

    # Mean absolute error
    MAE.pls.train = MAE(yactual.train, ypredCARET.pls.train)
    MAE.pls.train</code></pre>
<pre><code>## [1] 12.61364</code></pre>
<pre class="r"><code>    # Results in: 12.61

    # Plot ypred vs yactual of training data
    plot(yactual.train, ypredCARET.pls.train,
        xlab=&quot;Observed BP train set&quot;, ylab=&quot;Predicted BP train set&quot;,
        pch=19, xlim=c(0, ceiling(max(yNN)*1.1)), ylim=c(0, ceiling(max(yNN)*1.1)),main=&quot;Prediction error training set (PLS model)&quot;)
        abline(0,1, col=&#39;red&#39;)
        text(200,ceiling(max(yNN)*1.1),paste(&quot;RMSE: &quot;,RMSE.pls.train))
        text(200,ceiling(max(yNN)*1.1)-50,paste(&quot;MAE: &quot;,MAE.pls.train))</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-11-3.png" width="672" /></p>
</div>
<div id="evaluation-of-pls-model" class="section level4">
<h4>Evaluation of PLS model</h4>
<p>The 10-fold cross-validated PLS model used 3 latent variables (VAdjMat, Kier1, khs.ssCH2) to estimate the boiling point within 16 degrees kelvin, ranging between (rounded) 200 to 1100 degrees kelvin. The mean deviation is approximated to be <em>16 / (1100 - 200) x 100 =</em> 1.8%.</p>
<p>It can be observed the test RMSE is 16.55 and MAE 14.99; training RMSE is 16.08 and MAE 12.61.</p>
<p>If overfitting did happen, the training error would be significantly lower than the test error, which is not the case.</p>
<p>The training error is slightly lower than the test error, indicating the model fits dynamics of the data.</p>
<p>The MAE is comparable, indicative of no under- or overfitting.</p>
<p>By examining the plots and comparing the direction of error, it can be seen as the model balances the prediction; underestimating at first, then overestimating, then underestimating and at last overestimating. This might be induced by nonlinear effects which are not fully captured in the selected variables.</p>
</div>
<div id="script-block-12-random-forest-model" class="section level4">
<h4>Script block 12: RANDOM FOREST MODEL</h4>
<p>As the caret package includes many different machine learning methods, a random forest model has been included. Random forest has been chosen as this method has been proven powerful and can capture linear as well as nonliner effects. It can be observed the Random forest model’s performance is better than the PLS model. However, both models can be improved by tuning the parameters and obtaining better molecular descriptors.</p>
<pre class="r"><code>    # Define training control method; 10 - k - cross validation
    train_control &lt;- trainControl(method=&quot;cv&quot;, number=10)

    # Train the model
    model &lt;- train(y~., data=data.train, trControl=train_control, method=&quot;rf&quot;)

    # Find out what model is best
    print(model)</code></pre>
<pre><code>## Random Forest 
## 
## 106 samples
##  16 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 94, 97, 95, 96, 96, 95, ... 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE      Rsquared   MAE     
##    2    9.299976  0.9986546  7.147196
##    9    8.698776  0.9988164  6.703487
##   16    8.807106  0.9988158  6.798295
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was mtry = 9.</code></pre>
<pre class="r"><code>    # Find out most important variables - BLOCKED - this doesnt work for rf
    if(F){
        Varimportance = varImp(model)
        cat(paste(&quot;Best model fit with&quot;, model$bestTune, &quot;latent components \n&quot;))
        cat(paste(&quot;Latent components:&quot;,paste(rownames(Varimportance$importance)[order(decreasing = T,Varimportance$importance$Overall)][1:model$bestTune[1,]],collapse = &quot;, &quot;),&quot;\n&quot;))
        plot(Varimportance, main=&quot;Varible importance in rf model \n&quot;)
    }
    # Predict test set
    ypredCARET.rf.test &lt;- model %&gt;% predict(data.test)

    # Root mean squared error
    RMSE.rf.test = RMSE(yactual.test, ypredCARET.rf.test)
    RMSE.rf.test</code></pre>
<pre><code>## [1] 8.927311</code></pre>
<pre class="r"><code>    # Results in: 8.92

    # Mean absolute error
    MAE.rf.test = MAE(yactual.test, ypredCARET.rf.test)
    MAE.rf.test</code></pre>
<pre><code>## [1] 5.725729</code></pre>
<pre class="r"><code>    # Results in: 5.72

    # Plot ypred vs yactual of test data
    plot(yactual.test, ypredCARET.rf.test,
        xlab=&quot;Observed BP test set&quot;, ylab=&quot;Predicted BP test set&quot;,
        pch=19, xlim=c(0, ceiling(max(yNN)*1.1)), ylim=c(0, ceiling(max(yNN)*1.1)),main=&quot;Prediction error test set (RandomForest model)&quot;)
        abline(0,1, col=&#39;red&#39;)
        text(200,ceiling(max(yNN)*1.1),paste(&quot;RMSE: &quot;,RMSE.rf.test))
        text(200,ceiling(max(yNN)*1.1)-50,paste(&quot;MAE: &quot;,MAE.rf.test))</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre class="r"><code>    # Predict training data; check overfitting
    ypredCARET.rf.train &lt;- model %&gt;% predict(data.train)

    # Root mean squared error
    RMSE.rf.train = RMSE(yactual.train, ypredCARET.rf.train)
    RMSE.rf.train</code></pre>
<pre><code>## [1] 3.916698</code></pre>
<pre class="r"><code>    # Results in: 3.91

    # Mean absolute error
    MAE.rf.train = MAE(yactual.train, ypredCARET.rf.train)
    MAE.rf.train</code></pre>
<pre><code>## [1] 2.741942</code></pre>
<pre class="r"><code>    # Results in: 2.74

    # Plot ypred vs yactual of training data
    plot(yactual.train, ypredCARET.rf.train,
        xlab=&quot;Observed BP train set&quot;, ylab=&quot;Predicted BP train set&quot;,
        pch=19, xlim=c(0, ceiling(max(yNN)*1.1)), ylim=c(0, ceiling(max(yNN)*1.1)),main=&quot;Prediction error training set (RandomForest model)&quot;)
        abline(0,1, col=&#39;red&#39;)
        text(200,ceiling(max(yNN)*1.1),paste(&quot;RMSE: &quot;,RMSE.rf.train))
        text(200,ceiling(max(yNN)*1.1)-50,paste(&quot;MAE: &quot;,MAE.rf.train))</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-12-2.png" width="672" /></p>
</div>
<div id="evaluation-of-randomforest-model" class="section level4">
<h4>Evaluation of RandomForest model</h4>
<p>The 10-fold cross-validated RandomForest model used 9 predictors to estimate the boiling point within 8.3 degrees kelvin, ranging between (rounded) 200 to 1100 degrees kelvin. The mean deviation is approximated to be <em>8 / (1100 - 200) x 100 =</em> 0.9%.</p>
<p>It can be observed the test RMSE is 8.92 and MAE 5.72; training RMSE is 3.91 and MAE 2.74.</p>
<p>The the training error is lower than the test error, indicative of slight overfitting. However, the test set RMSE is 8.92, which is lower than the PLS model; indicative that this model outperforms the PLS model. Comparing the MAE indicates slight overfitting.</p>
<p>By examinating the plots and comparing the direction of error, it can be seen this prediction seems spot on with slight uniform error. Comparing this model to the PLS model a clear difference can be seen, which is likely caused by the nonlinear predictive power of random forest.</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
